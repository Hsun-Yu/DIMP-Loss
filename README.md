# Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification
Hsun-Yu Kuo, Yin-Hsiang Liao, Yu-Chieh Chao, Wei-Yun Ma, Pu-Jen Cheng

This repository contains the official implementation of the paper ["Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification"](https://openreview.net/forum?id=oI5tZaWkF9), presented as a **Spotlight** poster at **ICLR 2025**. The code builds upon the [Hugging Face Transformers library](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) to provide a robust framework for experimenting with advanced data weighting techniques in text classification tasks.

## Abstract
Synthetic data augmentation via Large Language Models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring deficient outcomes while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs with using merely a little real-world data. We empirically assessed the effectiveness of our methods on multiple text classification tasks, and the results showed leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator.

## Requirements
```bash
conda env create -f environment.yml
conda activate dimp
```
## Fine-tuning with DIMP-Loss for MRPC

To train the BERT model with DIMP-Loss, you can either use your own trained quality checker model or the one we provide. If using your own model, update the `twomodelloss_wandb_model2` parameter with the corresponding W&B artifact name. Ensure the quality checker model is uploaded as an artifact to W&B.

Alternatively, to use our provided quality checker model, simply run:

```bash
python run.py --config configs/config_DIMP.json
```
This command will automatically download the quality checker model from W&B and train the BERT model with DIMP-Loss using the configuration specified in `configs/config_DIMP.json`.

## Fine-tuning with IMP-Loss for MRPC

To use your own trained models as the quality checker and diversity checker, update the `twomodelloss_wandb_model2` and `twomodelloss_wandb_model3` parameters, respectively. Ensure both models are uploaded as artifacts to W&B. 

Alternatively, to use our provided quality checker and diversity checker models, run the following command:

```bash
python run.py --config configs/config_IMP.json
```

This command will automatically download the provided quality checker and diversity checker models from W&B and train the BERT model with IMP-Loss using the configuration specified in `configs/config_IMP.json`.

## Fine-tuning with CE-Loss (baseline) for MRPC

```bash
python run.py --config configs/config_baseline.json
```

## Important Parameters

- **`model_name_or_path`**:  
  Specifies the pretrained model path or identifier from Hugging Face's model hub (e.g., `bert-base-uncased`, `vinai/bertweet-base`, `hsunyu/epfl_ml_project2/twitter_full_bertweet_large:v1`).
  If using a W&B model, set `use_wandb_model` to `True` and specify the model name in the `wandb_model` key.

- **`problem_type`**:  
  Defines the task type. Examples include:
  - `"single_label_classification"`: For text classification with cross-entropy loss (CE-Loss).
  - `"single_label_classification_myloss_v2"`: For the DIMP-Loss approach.
  - `"single_label_classification_myloss_importance"`: For the IMP-Loss approach.

- **`wandb_dataset`**:  
  Specifies the W&B dataset artifact name for training and evaluation. Examples:
  - `hsunyu/NLP_Data_Augmentation/datapoint_induction_glue_mrpc:v0`: LLM-generated data for MRPC benchmark.

- **`use_wandb_model`**:  
  Boolean indicating whether to load a pretrained model from a W&B artifact. Useful for reproducibility. (This one must be set to `True` in this repo.)

- **`twomodelloss_wandb_model2`**:  
  Refers to the W&B artifact for the quality checker model used in DIMP-Loss training. Example: `hsunyu/NLP_Data_Augmentation/only_valid_glue_mrpc_bert:v0`.

  - **`twomodelloss_wandb_model3`**:  
  Refers to the W&B artifact for the quality checker model used in DIMP-Loss training. Example: `hsunyu/NLP_Data_Augmentation/few-shot_glue_mrpc_bert_5:v0`.

- **`per_device_train_batch_size`**:  
  Defines the batch size per device during training. Default: `128`.

- **`num_train_epochs`**:  
  Specifies the total number of training epochs. Default: `3.0`.

## Citation
If you find our work or code useful in your research, you could cite those with following Bibtex:

```bibtex
@inproceedings{
kuo2025not,
title={Not All {LLM}-Generated Data Are Equal: Rethinking Data Weighting in Text Classification},
author={Hsun-Yu Kuo and Yin-Hsiang Liao and Yu-Chieh Chao and Wei-Yun Ma and Pu-Jen Cheng},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=oI5tZaWkF9}
}
```

## Acknowledgements
